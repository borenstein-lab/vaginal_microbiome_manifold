{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "205676c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import pandas as pd\n",
    "import xlsxwriter\n",
    "import openpyxl\n",
    "import os\n",
    "import numpy as np\n",
    "import itertools\n",
    "import itertools as it\n",
    "from pandas import Series, ExcelWriter\n",
    "import csv\n",
    "from skbio.stats.composition import clr\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb70d15",
   "metadata": {},
   "source": [
    "#### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02950043",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_path = '../SPECIFIC_PAPERS/'\n",
    "\n",
    "db_path1 = top_path + 'Ravel_et_al.2013/Hiseq_sequencing/DADA2_results/'\n",
    "db_path2 = top_path + 'Ravel_et_al.2013/454_Sequencing/DADA2_results/DADA_output_files/'\n",
    "db_path3 = top_path + 'Carter_et_al.2022/DADA2_results/'\n",
    "db_path4 = top_path + 'Ravel_et_al.2011/DADA2_results/'\n",
    "db_path5 = top_path + 'Serrano_et_al.2019/noDuplicatedID/DADA2_results/'\n",
    "db_path6 = top_path + 'Srinivasan_et_al.2012/DADA2_results/'\n",
    "\n",
    "asv_abun_file_name = 'abundance_ASV.csv'\n",
    "taxa_file_name = 'taxa_ASV.csv'\n",
    "\n",
    "speciate_filename1 = 'Ravel2013_Hiseq_2step_single_V3V4_MC_order7.txt'\n",
    "speciate_filename2 = 'Ravel2013_V1V3_454_all_MC_order7_rev.txt'\n",
    "speciate_filename3 = 'Carter2022_V3V4_MC_order7.txt'\n",
    "speciate_filename4 = 'Ravel2011_V1V3_MC_order7.txt'\n",
    "speciate_filename5 = 'Serrano2019_MC_order7.txt'\n",
    "speciate_filename6 = 'Srinivasan2012_V3V4_MC_order7.txt'\n",
    "\n",
    "taxa_names_lst = ['k','p','c','o','f','g','s']\n",
    "names_lst = ['Lactobacillus', 'Gardnerella', 'Prevotella', 'Atopobium', 'Shuttleworthia', 'Sneathia', 'BVAB', 'Unknown']\n",
    "date = '16052023'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f5820f",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a01f7aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_tax(df):\n",
    "    new_df = df.copy()\n",
    "    new_df = df.fillna(0)\n",
    "    new_df.columns = taxa_names_lst\n",
    "    new_df['cond_spec'] = np.nan\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "841ebba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cond(row):\n",
    "    if type(row['s']) == str:\n",
    "        row['cond_spec'] = row['g'] + '_' + row['s']\n",
    "    \n",
    "    else:\n",
    "        for i in range(len(taxa_names_lst)):\n",
    "            curr_level = taxa_names_lst[i]\n",
    "            if row[curr_level] == 0:\n",
    "                if curr_level == 'k':\n",
    "                    row['cond_spec'] = np.nan\n",
    "                else:\n",
    "                    upper_level = taxa_names_lst[i - 1]\n",
    "                    row['cond_spec'] = upper_level + '_' + row[upper_level]\n",
    "                break\n",
    "    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbc95d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_speciate(df):\n",
    "    df.columns = ['ASV', 'speciate_spec', 'posterior_probability', 'decisions_num']\n",
    "    df.drop(['posterior_probability', 'decisions_num'], axis = 1, inplace = True)\n",
    "    df.set_index('ASV', inplace = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c68ebe4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_final_name(row):\n",
    "    if any(name in row['speciate_spec'] for name in names_lst) or 'BVAB' in row['speciate_spec']:\n",
    "        return row['speciate_spec']\n",
    "    else:\n",
    "        if row['g'] is not 'Unknown' and '00' not in row['g']:\n",
    "            return 'g_' + row['g']\n",
    "        elif row['f'] != 0:\n",
    "            return 'f_' + row['f']\n",
    "        elif row['s'] == 0: \n",
    "            return row['cond_spec']\n",
    "        else:\n",
    "            return 'Unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84b7be19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_name(name):\n",
    "    if name.split('_')[0] == 'f':\n",
    "        return name\n",
    "    elif 'Cluster' in name:\n",
    "        return name\n",
    "    elif len(name.split('_')[0]) == 1 and name.split('_')[0] != 'f':\n",
    "        return '_'.join(name.split('_')[:2])\n",
    "    elif any(map(str.isdigit, name.split('_')[1])):\n",
    "        return '_'.join([name.split('_')[i] for i in [0, 2]])\n",
    "    else: \n",
    "        return '_'.join(name.split('_')[-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d350117b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spec_df(path, spec_path):\n",
    "    ## DADA tax df\n",
    "    tax_df_orig = pd.read_csv(path + taxa_file_name, index_col = 0)\n",
    "    print('Tax_df row num: ' + str(tax_df_orig.shape[0]))\n",
    "    \n",
    "    tax_df = order_tax(tax_df_orig)\n",
    "    tax_df = tax_df.apply(lambda row: get_cond(row), axis = 1)\n",
    "    tax_df['g'] = tax_df['g'].replace({0: 'Unknown'})\n",
    "    \n",
    "    ## Speciate df\n",
    "    speciate_df = pd.read_csv(path + spec_path, sep = '\\t', header = None)\n",
    "    speciate_df = order_speciate(speciate_df)\n",
    "    \n",
    "    ## Merge both dfs\n",
    "    merged_tax_df = tax_df.join(speciate_df)\n",
    "    \n",
    "    ## Create final name column\n",
    "    merged_tax_df['final_name'] = merged_tax_df.apply(add_final_name, axis = 1)\n",
    "    \n",
    "    ## Replace long names\n",
    "    long_names_lst = [name for name in merged_tax_df.final_name.unique() if type(name) == str and name.count('_') > 1]\n",
    "    \n",
    "    new_names = []\n",
    "    for name in long_names_lst:\n",
    "        new_name = get_new_name(name)\n",
    "        new_names.append(new_name)\n",
    "        \n",
    "    replacers = dict(zip(long_names_lst, new_names))\n",
    "    replacers.update({'Lactobacillus_ultunensis': 'Lactobacillus_crispatus'})\n",
    "\n",
    "    merged_tax_df['final_name'] = merged_tax_df['final_name'].replace(replacers)\n",
    "    merged_tax_df = merged_tax_df.loc[(merged_tax_df['p'] != 0)]\n",
    "    print('Merged tax df after removal of k_ ASVs: ' + str(merged_tax_df.shape[0]))\n",
    "    \n",
    "    long_names_lst = [name for name in merged_tax_df.final_name.unique() if name.count('_') > 1]\n",
    "    \n",
    "    return merged_tax_df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8e025c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spec_reads_table(path, merged_tax_df):\n",
    "    abun_df = pd.read_csv(path + asv_abun_file_name, index_col = 0)\n",
    "    print('Orig reads table columns: ' + str(abun_df.shape[1]))\n",
    "    \n",
    "    filt_abun_df = abun_df[merged_tax_df.index]\n",
    "    abun_df_t = filt_abun_df.T\n",
    "    spec_abun_df = pd.merge(merged_tax_df[['final_name']], abun_df_t, left_index = True, right_index = True)\n",
    "\n",
    "    spec_abun_df.reset_index(drop = True, inplace = True)\n",
    "    spec_abun_df.set_index('final_name', inplace = True)\n",
    "    spec_abun_df.index.name = None\n",
    "    spec_abun_df = spec_abun_df.T\n",
    "    print('Final reads table columns: ' + str(spec_abun_df.shape[1]))\n",
    "        \n",
    "    return spec_abun_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0b5bb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filtered(df):\n",
    "    filt_df = df.loc[df.sum(axis = 1) >= 1000]\n",
    "    filt_df02 = filt_df.loc[:, filt_df.sum(axis = 0) >= 100]\n",
    "    filt_df03 = filt_df02.drop(filt_df02.loc[filt_df02.sum(axis = 1) == 0].index, inplace = False)\n",
    "    \n",
    "    print('Shape after filter- rows: ' + str(filt_df03.shape[0]) + ', columns: ' + str(filt_df03.shape[1]))\n",
    "\n",
    "    return filt_df03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47d20fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_spec(df):\n",
    "    sum_spec_df = df.groupby(df.columns, axis = 1).sum()\n",
    "    print('Sum of species columns: ' + str(sum_spec_df.shape[1]))\n",
    "    \n",
    "    return sum_spec_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae01a2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valencia_input(df):   \n",
    "    valencia_df = df.copy()\n",
    "    valencia_df.reset_index(inplace = True)\n",
    "    valencia_df = valencia_df.rename(columns = {'index':'sampleID'})\n",
    "    read_count_values = valencia_df.sum(axis = 1)\n",
    "    valencia_df.insert(loc = 1, column = 'read_count', value = read_count_values)\n",
    "    \n",
    "    return valencia_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bd411c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_and_cmd_valencia(path, df):\n",
    "    valencia_file_name = 'valencia_df_' + date \n",
    "    df.to_csv(path + valencia_file_name + '.csv', index = False)\n",
    "    \n",
    "    in_file = path + valencia_file_name + '.csv'\n",
    "    out_file = path + valencia_file_name + '_res'\n",
    "    cmd = 'py C:/Users/morts/Documents/TAU/Python_Scripts/Manifold_Project/Valencia/Valencia.py -ref C:/Users/morts/Documents/TAU/Taxonomy/CST_centroids_19042023.csv -i ' + in_file + ' -o ' + out_file\n",
    "    print('*** cmd for VALENCIA ***')\n",
    "    print(cmd)\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38b10779",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_func(name, path, spec_path):\n",
    "    ## Table with ASV and taxa final\n",
    "    merged_tax_df = get_spec_df(path, spec_path)\n",
    "    \n",
    "    ## Reads table with taxa names (after sum)\n",
    "    spec_abun_df = get_spec_reads_table(path, merged_tax_df)\n",
    "    \n",
    "    ## Filtering\n",
    "    filt_df = get_filtered(spec_abun_df)\n",
    "    sum_spec_df = sum_spec(filt_df)\n",
    "    \n",
    "    ## VALENCIA input\n",
    "    input_valencia = get_valencia_input(sum_spec_df)\n",
    "#     save_and_cmd_valencia(path, input_valencia)\n",
    "    \n",
    "    print('****  ****  ****')\n",
    "    \n",
    "    return merged_tax_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac23f88",
   "metadata": {},
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04ddba8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_lst = [db_path1, db_path2, db_path3, db_path4, db_path5, db_path6]\n",
    "speciate_lst = [speciate_filename1, speciate_filename2, speciate_filename3, speciate_filename4, speciate_filename5, speciate_filename6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9623dd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tax_df row num: 22633\n",
      "Merged tax df after removal of k_ ASVs: 17495\n",
      "Orig reads table columns: 22633\n",
      "Final reads table columns: 17495\n",
      "Shape after filter- rows: 3412, columns: 8638\n",
      "Sum of species columns: 278\n",
      "****  ****  ****\n",
      "***\n",
      "Tax_df row num: 6676\n",
      "Merged tax df after removal of k_ ASVs: 6545\n",
      "Orig reads table columns: 6676\n",
      "Final reads table columns: 6545\n",
      "Shape after filter- rows: 1573, columns: 1516\n",
      "Sum of species columns: 82\n",
      "****  ****  ****\n",
      "***\n",
      "Tax_df row num: 3598\n",
      "Merged tax df after removal of k_ ASVs: 3544\n",
      "Orig reads table columns: 3598\n",
      "Final reads table columns: 3544\n",
      "Shape after filter- rows: 706, columns: 1576\n",
      "Sum of species columns: 137\n",
      "****  ****  ****\n",
      "***\n",
      "Tax_df row num: 4851\n",
      "Merged tax df after removal of k_ ASVs: 4840\n",
      "Orig reads table columns: 4851\n",
      "Final reads table columns: 4840\n",
      "Shape after filter- rows: 385, columns: 875\n",
      "Sum of species columns: 54\n",
      "****  ****  ****\n",
      "***\n",
      "Tax_df row num: 37719\n",
      "Merged tax df after removal of k_ ASVs: 37240\n",
      "Orig reads table columns: 37719\n",
      "Final reads table columns: 37240\n",
      "Shape after filter- rows: 3879, columns: 7835\n",
      "Sum of species columns: 278\n",
      "****  ****  ****\n",
      "***\n",
      "Tax_df row num: 604\n",
      "Merged tax df after removal of k_ ASVs: 594\n",
      "Orig reads table columns: 604\n",
      "Final reads table columns: 594\n",
      "Shape after filter- rows: 185, columns: 170\n",
      "Sum of species columns: 42\n",
      "****  ****  ****\n",
      "***\n"
     ]
    }
   ],
   "source": [
    "for i, path in enumerate(db_lst):\n",
    "    df = all_func('test', db_lst[i], speciate_lst[i])\n",
    "    print('***')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
